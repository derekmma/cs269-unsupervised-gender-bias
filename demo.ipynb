{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431601d5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this work is to detect subtle gender biases like microaggresions, objectifications and condescenision in 2nd person text. Espeically the biased comment normally associated with previous context, so current classifiers that detect hate speech, offensive language, or negative sentiment cannot detect these comments. \n",
    "\n",
    "Some example biased sentences are:\n",
    "* \"Oh, you work at an office? I bet you're a secretary\"\n",
    "* \"Total tangent I know, but you're gorgeous\"\n",
    "\n",
    "We can see that in these examples, the second segments alone may not be biased, but when we put them in context, they become problematic.\n",
    "\n",
    "If we could have a automated detection classifier that could detect such biases sentence, that would be beneficial to:\n",
    "* Make the posters aware of the bias so they will not post them at the first place (proactive response)\n",
    "* Flag harm comments so readers can filter out biased comments (active response)\n",
    "\n",
    "A straightforward way is to train a supervised classifier if we have statement and bias label. But the biases are subtle and implicit, even experts are bad at identifying them. It is quite expensive to collect an annotation dataset with a reasonable scale.\n",
    "\n",
    "In this work, the authors proposed to use an unsupervised method. They proposed to first train a classifier that predicts that gender of the person that the text is addressed to (whether the addressee is male or female). If the classifier makes a prediction with high confidence, the text likely contains bias.\n",
    "\n",
    "But a significant challenge when using this unsupervised method is that confounds in the input text may contain other aspects of information (which is not indicative of bias) that can lead to high-confidence classifier prediction. For example, if the comment is \"Bro, golf is better\", then the word \"bro\" can easily indicate the addressee is male but there is no bias. \n",
    "\n",
    "To deal with this challenge, this work further propose methods to control confounding variables when training the gender bias classifier.\n",
    "\n",
    "To summarize, the general idea of this paper is that comments contain bias if they are **highly predictive** of gender **despite confound control**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cbdb54",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Each data point contains the following variables:\n",
    "* OW\n",
    "\n",
    "[TODO] more about data point, all variables\n",
    "[TODO] more about the facebook dataset with some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3253910",
   "metadata": {},
   "source": [
    "# Method: Classifier for Addressee Gender Prediction\n",
    "\n",
    "The input to the prediction model is the following comment, the ouput is the gender of the addressee. we aim to identify bias in the following comment. The authors proposed two methods to control confounds from different perspective:\n",
    "* Observed confounding variables are balanced through propensity matching\n",
    "* Latent confounding variables are demoted through adversarial training\n",
    "\n",
    "## Controlling Observed Confounding Variables through Propnesity Matching\n",
    "\n",
    "In our problem statement, comments are written in reply to \"original text\" writtedn by the addressee. Then another writer wrote \"comment\" to reply to the addressee. Comment content is influenced by both the \"original text\" and the potential bias factor such as the gender of the addressee. This first method for controlling confounds targets to reduce the influence from the \"original text\".\n",
    "\n",
    "The primary method to this is using propensity matching. We discard any comment text training samples whose associated original text is healvily affiliate with only one gender. The goal is to balance the dataset, so that the comment text from male and female has similar probabilities to associate with the original text.\n",
    "\n",
    "But the problem is it's hard to find comments written by male and females with identical original text in practice (that's why there is bias in the dataset!) \n",
    " \n",
    "So the propensity score for a comment text is defined as the probability that the writter is female given the original text.Â \n",
    "\n",
    "## Controlling Latent Confounding Variables through Adversarial Training\n",
    "\n",
    "Comments may also influenced by traits of the addressee such as occupation, nationality, nicknames etc other than gender. These additional factors are unique to individuals and there are many of them, so it's hard to enumerate all of them. We also want to control influence from these other confounding factors other than gender.\n",
    "\n",
    "Traits are inferred from comments using log-odds scores and represented in a vector. The GAN-like training procedure discourages the model from learning these traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3032691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/miniconda3/envs/cs269/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "usage: ipykernel_launcher.py [-h] [--data {RT_GENDER,RT_GENDER_OP_POSTS}]\n",
      "                             --base_path BASE_PATH [--test_file TEST_FILE]\n",
      "                             [--ood_test_file OOD_TEST_FILE]\n",
      "                             [--train_file TRAIN_FILE]\n",
      "                             [--valid_file VALID_FILE] [--rnn_model RNN_MODEL]\n",
      "                             [--save_dir SAVE_DIR] [--model MODEL]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--topic_loss TOPIC_LOSS] [--emsize EMSIZE]\n",
      "                             [--hidden HIDDEN] [--nlayers NLAYERS]\n",
      "                             [--num_topics NUM_TOPICS] [--lr LR] [--clip CLIP]\n",
      "                             [--epochs EPOCHS] [--gpu GPU] [--alpha ALPHA]\n",
      "                             [--batch_size N] [--drop DROP] [--gradreverse]\n",
      "                             [--bi] [--save_output_topics]\n",
      "                             [--output_topics_save_filename OUTPUT_TOPICS_SAVE_FILENAME]\n",
      "                             [--cuda] [--load] [--latest] [--write_attention]\n",
      "                             [--write_predictions] [--demote_topics]\n",
      "                             [--onlytopics] [--alpha_sched] [--alpha_sched2]\n",
      "                             [-kernel-num KERNEL_NUM]\n",
      "                             [-kernel-sizes KERNEL_SIZES]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --base_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/miniconda3/envs/cs269/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import codecs\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('./src'))\n",
    "\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from datasets import make_rt_gender, make_rt_gender_op_posts\n",
    "from model import *\n",
    "from torchtext.vocab import GloVe\n",
    "from train import *\n",
    "\n",
    "topic_criterion = nn.KLDivLoss(size_average=False)\n",
    "# topic_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "pretrained_GloVe_sizes = [50, 100, 200, 300]\n",
    "\n",
    "args = make_parser().parse_args()\n",
    "print(\"[Model hyperparams]: {}\".format(str(args)))\n",
    "\n",
    "cuda = torch.cuda.is_available() and args.cuda\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "# device = torch.device(\"cpu\") if not cuda else torch.device(\"cuda:\"+str(args.gpu))\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "seed_everything(seed=1337, cuda=cuda)\n",
    "vectors = None #don't use pretrained vectors\n",
    "# vectors = load_pretrained_vectors(args.emsize)\n",
    "\n",
    "# Load dataset iterators\n",
    "if args.data == \"RT_GENDER\":\n",
    "    iters, TEXT, LABEL, INDEX = make_rt_gender(args.batch_size, base_path=args.base_path, train_file=args.train_file, valid_file=args.valid_file, test_file=args.test_file, device=-1, vectors=vectors, topics=False)\n",
    "    train_iter, val_iter, test_iter = iters\n",
    "elif args.data == \"RT_GENDER_OP_POSTS\":\n",
    "    iters, TEXT, LABEL, INDEX = make_rt_gender_op_posts(args.batch_size, base_path=args.base_path, train_file=args.train_file, valid_file=args.valid_file, test_file=args.test_file, device=-1, vectors=vectors)\n",
    "    if len(iters) == 2:\n",
    "        train_iter, test_iter = iters\n",
    "        val_iter = test_iter\n",
    "    else:\n",
    "        train_iter, val_iter, test_iter = iters\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "print(\"[Corpus]: train: {}, test: {}, vocab: {}, labels: {}\".format(\n",
    "        len(train_iter.dataset), len(test_iter.dataset), len(TEXT.vocab), len(LABEL.vocab)))\n",
    "\n",
    "if args.model == \"CNN\":\n",
    "    args.embed_num = len(TEXT.vocab)\n",
    "    args.nlabels = len(LABEL.vocab)\n",
    "    args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n",
    "    args.embed_dim = args.emsize\n",
    "\n",
    "    model = CNN_Text(args, num_topics=args.num_topics)\n",
    "\n",
    "elif args.model == \"FFN_BOW\":\n",
    "    args.embed_num = len(TEXT.vocab)\n",
    "    args.nlabels = len(LABEL.vocab)\n",
    "    args.embed_dim = args.emsize\n",
    "\n",
    "    model = FFN_BOW_Text(args)\n",
    "\n",
    "elif args.model == \"FFN\":\n",
    "    args.nlabels = len(LABEL.vocab)\n",
    "    model = FFN_Text(args)\n",
    "\n",
    "else:\n",
    "    ntokens, nlabels = len(TEXT.vocab), len(LABEL.vocab)\n",
    "    args.nlabels = nlabels # hack to not clutter function arguments\n",
    "\n",
    "    embedding = nn.Embedding(ntokens, args.emsize, padding_idx=1)\n",
    "    if vectors: embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "    encoder = Encoder(args.emsize, args.hidden, nlayers=args.nlayers,\n",
    "                      dropout=args.drop, bidirectional=args.bi, rnn_type=args.rnn_model)\n",
    "\n",
    "    attention_dim = args.hidden if not args.bi else 2*args.hidden\n",
    "    attention = BahdanauAttention(attention_dim, attention_dim)\n",
    "\n",
    "    model = Classifier(embedding, encoder, attention, attention_dim, nlabels, num_topics=args.num_topics)\n",
    "    print('model initialized')\n",
    "\n",
    "model.to(device)\n",
    "print('Moved model to device, ', device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# topic_criterion = nn.KLDivLoss(size_average=False)\n",
    "topic_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), args.lr, amsgrad=True)\n",
    "\n",
    "print('A')\n",
    "if not args.load:\n",
    "    for p in model.parameters():\n",
    "        if not p.requires_grad:\n",
    "            print (\"OMG\", p)\n",
    "            p.requires_grad = True\n",
    "        p.data.uniform_(-0.5, 0.5)\n",
    "    # print (p.data.norm())\n",
    "\n",
    "print('B')\n",
    "# trainloss = evaluate(best_model, train_iter, optimizer, criterion, args, datatype='train', writetopics=args.save_output_topics, itos=TEXT.vocab.itos)\n",
    "if args.load:\n",
    "    print(args.save_dir+\"/\"+args.model_name+\"_bestmodel\")\n",
    "    if args.latest:\n",
    "        best_model = torch.load(args.save_dir+\"/\"+args.model_name+\"_latestmodel\")\n",
    "    else:\n",
    "        print('C')\n",
    "        # best_model = torch.load(args.save_dir+\"/\"+args.model_name+\"_bestmodel\")\n",
    "        best_model = torch.load(args.save_dir+\"/\"+args.model_name+\"_bestmodel\", map_location=torch.device(device))\n",
    "    print('saved model loaded')\n",
    "else:\n",
    "    try:\n",
    "        best_valid_loss = None\n",
    "        best_model = None\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train(model, train_iter, optimizer, criterion, args, epoch)\n",
    "            loss = evaluate(model, val_iter, optimizer, criterion, args)\n",
    "\n",
    "            if not best_valid_loss or loss < best_valid_loss:\n",
    "                best_valid_loss = loss\n",
    "                print (\"Updating best model\")\n",
    "                best_model = copy.deepcopy(model)\n",
    "                torch.save(best_model, args.save_dir+\"/\"+args.model_name+\"_bestmodel\")\n",
    "            torch.save(model, args.save_dir+\"/\"+args.model_name+\"_latestmodel\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[Ctrl+C] Training stopped!\")\n",
    "\n",
    "if not args.load:\n",
    "    trainloss = evaluate(best_model, train_iter, optimizer, criterion, args, datatype='train', writetopics=args.save_output_topics, itos=TEXT.vocab.itos, litos=LABEL.vocab.itos)\n",
    "    valloss = evaluate(best_model, val_iter, optimizer, criterion, args, datatype='valid', writetopics=args.save_output_topics, itos=TEXT.vocab.itos, litos=LABEL.vocab.itos)\n",
    "print('start evaluating...')\n",
    "loss = evaluate(best_model, test_iter, optimizer, criterion, args, datatype=os.path.basename(args.test_file).replace(\".txt\", \"\").replace(\".tsv\", \"\"), writetopics=args.save_output_topics, itos=TEXT.vocab.itos, litos=LABEL.vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "################################## TO FILL IN ############################################################\n",
    "# source activate your-env-here\n",
    "\n",
    "TOP_DIR=\"/home/ma/fairness/unsupervised_gender_bias_models\" # the path to unzipped tarball of saved models\n",
    "\n",
    "SUFFIX=\"facebook_wiki\"  # Flip comment to change which data set to run\n",
    "# SUFFIX=\"facebook_congress\"\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "# Paths to data\n",
    "DATA_DIR=\"${TOP_DIR}/${SUFFIX}\"\n",
    "\n",
    "# Intermediate suffixes\n",
    "MATCHED_SUFFIX=\"matched_${SUFFIX}\"\n",
    "SUBS_SUFFIX=\"subs_name2\"\n",
    "EXTRA_SUFFIX=\"withtopics\"\n",
    "NO_SUFFIX=\"notopics\"\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "python -m src.train.py --data RT_GENDER --base_path ${DATA_DIR} --train_file train.${SUBS_SUFFIX}.${MATCHED_SUFFIX}.${NO_SUFFIX}.txt --valid_file valid.${SUBS_SUFFIX}.${SUFFIX}.txt --test_file test.${SUBS_SUFFIX}.${SUFFIX}.txt --save_dir ${DATA_DIR}/matched_notopics_${SUBS_SUFFIX} --model RNN --model_name rt_gender_${SUFFIX}_matched_notopics.model --gpu 0 --batch_size 32  --write_attention --epochs 5 --lr 0.0001 --load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd4543",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb4163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs269)",
   "language": "python",
   "name": "cs269"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
